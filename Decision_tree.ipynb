{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1** - What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "**Answer** - A Decision Tree is a flowchart-like model used in machine learning to make decisions or predictions. In the context of classification, it helps sort data into categories (like \"spam\" or \"not spam\") by asking a series of yes/no or multiple-choice questions about the input features.\n",
        "\n",
        "It starts at a root node and splits the data based on the feature that best separates the classes. Each internal node asks a question, and each branch represents an answer. The process continues until it reaches a leaf node, which gives the final classification."
      ],
      "metadata": {
        "id": "AGzD3aoqKQWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2** - Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "**Answer** - Gini Impurity and Entropy are two commonly used measures in decision trees to decide how \"pure\" or \"impure\" a node is ‚Äî that is, how mixed the classes are at that point.\n",
        "\n",
        " 1. Gini Impurity:\n",
        "\n",
        "Measures the probability of misclassifying a randomly chosen element.\n",
        "\n",
        "Formula:\n",
        "\n",
        "                           **Gini=1‚àí‚àë(pi‚Äã)2**\n",
        "\n",
        "where\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " is the probability of class\n",
        "ùëñ\n",
        "i.\n",
        "\n",
        "Range: 0 (pure) to ~0.5 (most impure with two classes).\n",
        "\n",
        "Faster to compute than entropy.\n",
        "\n",
        " 2. Entropy:\n",
        "\n",
        "Measures the level of disorder or uncertainty in the node.\n",
        "\n",
        "Formula:\n",
        "\n",
        "                         **Entropy=‚àí‚àëpi‚Äãlog2‚Äã(pi‚Äã)**\n",
        "\n",
        "Range: 0 (pure) to 1 (for a 50/50 split in binary classification).\n",
        "\n",
        "Based on information theory.\n",
        "\n",
        "-  How They Impact Splits in a Decision Tree:\n",
        "\n",
        "At each node, the tree tries different features and thresholds and chooses the split that reduces impurity the most (i.e., has the biggest information gain for entropy, or Gini decrease).\n",
        "\n",
        "This helps the tree grow in a way that groups similar items together more effectively.\n"
      ],
      "metadata": {
        "id": "qqr40iq5Ms47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**- What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer - **Pre-Pruning**\n",
        "\n",
        "**Definition**: Pre-pruning (also called early stopping) is the process of stopping the growth of the decision tree before it becomes too complex, based on certain conditions (like maximum depth, minimum samples per leaf, or minimum information gain).\n",
        "\n",
        "**Post-Pruning**\n",
        "\n",
        "**Definition**: Post-pruning is the process of growing the full decision tree first and then removing or trimming the less significant branches after the tree is built, to reduce overfitting.\n",
        "\n",
        "**Practical Advantage**:\n",
        "\n",
        "Pre-Pruning: It helps reduce computation time and memory usage because the tree is never allowed to grow too large.\n",
        "\n",
        "Post-Pruning: It often results in a more accurate and generalized model, as pruning removes branches that do not contribute much to prediction accuracy."
      ],
      "metadata": {
        "id": "exMH8Bx1ONn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4** - What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Answer - **Information Gain in Decision Trees:**\n",
        "\n",
        "**Definition**:\n",
        "Information Gain is a measure used in decision trees to determine how well a particular feature separates the data into target classes. It calculates the reduction in entropy (or uncertainty) about the target variable after splitting the dataset based on a specific attribute.\n",
        "\n",
        "In simple terms, it tells us how much ‚Äúinformation‚Äù about the target class we gain by splitting the data on a given feature.\n",
        "\n",
        " **Importance for Choosing the Best Split**:\n",
        "\n",
        "Information Gain is important because it helps the decision tree choose the most informative attribute at each node.\n",
        "\n",
        "A higher Information Gain means the feature reduces more uncertainty and creates purer subsets.\n",
        "\n",
        "The decision tree algorithm selects the feature with the highest Information Gain as the best split, leading to a more accurate and efficient tree."
      ],
      "metadata": {
        "id": "e_EFymKxRhtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5** - What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "**Answer** - **Real-World Applications of Decision Trees**\n",
        "\n",
        "Decision trees are used in many practical situations because they are simple to understand and easy to apply. For example, in medical diagnosis, they help doctors predict diseases or suggest treatments based on patient data. In finance and banking, they assist with credit scoring, loan approvals, and detecting fraudulent activities. Companies also use decision trees in marketing to segment customers, predict their purchasing behavior, and plan targeted campaigns. In manufacturing, they can identify causes of defects or predict machine failures for better maintenance planning. Even in education, decision trees are useful for predicting student performance or identifying those at risk of dropping out.\n",
        "\n",
        " **Main Advantages of Decision Trees**\n",
        "\n",
        "One major advantage is that decision trees are easy to understand and interpret because their results can be visualized like a simple flowchart. They also don‚Äôt require data scaling or complex preprocessing, which makes them easier to use. Another benefit is that they can handle both categorical and numerical data without any modifications. Additionally, decision trees help in identifying the most important features in a dataset, which is valuable for analysis and feature selection.\n",
        "\n",
        "**Main Limitations of Decision Trees**\n",
        "\n",
        "Despite their usefulness, decision trees have some limitations. They are prone to overfitting, meaning they can become too complex and learn noise from the training data. They can also be unstable, as small changes in the dataset might result in a completely different tree structure. Another issue is that decision trees can be biased towards features with many levels. Lastly, a single decision tree often performs less accurately compared to ensemble methods like Random Forests or Gradient Boosted Trees."
      ],
      "metadata": {
        "id": "3yc1GjAKSIXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6** -  Write a Python program to:\n",
        "\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "‚óè Print the model‚Äôs accuracy and feature importances"
      ],
      "metadata": {
        "id": "ujBc-3VbRCIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer - # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8lC5mk1S6wf",
        "outputId": "c204ac88-7029-413a-a3a6-2d43464c1b7b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "We load the Iris dataset using load_iris().\n",
        "\n",
        "We split it into training and testing data.\n",
        "\n",
        "We train a decision tree classifier using the Gini impurity criterion.\n",
        "\n",
        "Finally, we print the accuracy of the model and the feature importances, which tell us how much each feature contributed to the decision-making process."
      ],
      "metadata": {
        "id": "A9blM2rsUhbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7** -  Write a Python program to:\n",
        "\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "7zhSCov2Tjgs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7-NVKMTwKKNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d269464-6eeb-4325-b698-108d19e0d098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully-grown tree: 1.0\n"
          ]
        }
      ],
      "source": [
        "#Answer - # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Classifier with max_depth = 3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# 4. Train a fully-grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)  # no depth limit\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 5. Print and compare accuracies\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_limited)\n",
        "print(\"Accuracy with fully-grown tree:\", accuracy_full)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "We first load the Iris dataset and split it into training and testing data.\n",
        "\n",
        "Then we train one decision tree with a limited depth (max_depth=3) and another without depth limitation (fully-grown).\n",
        "\n",
        "Finally, we compare their accuracies on the test set."
      ],
      "metadata": {
        "id": "YnBsFK9LUpW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8** -  Write a Python program to:\n",
        "\n",
        "‚óè Load the California Housing dataset from sklearn\n",
        "\n",
        "‚óè Train a Decision Tree Regressor\n",
        "\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances\n"
      ],
      "metadata": {
        "id": "GHUs9oAAUy0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer - # Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Calculate and print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Gf3pWbUY_d",
        "outputId": "7eaecef4-7a16-49cd-dcc2-e28350bca548"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "The dataset is loaded with fetch_california_housing().\n",
        "\n",
        "We split the data into training and testing sets.\n",
        "\n",
        "A Decision Tree Regressor is trained to predict housing prices.\n",
        "\n",
        "The Mean Squared Error (MSE) evaluates how close the predictions are to actual values (lower = better).\n",
        "\n",
        "Feature importances show which features contribute most to the prediction ‚Äî here, median income (MedInc) is the most important."
      ],
      "metadata": {
        "id": "svlyLSJPVS27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9** -  Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "‚óè Print the best parameters and the resulting model accuracy\n"
      ],
      "metadata": {
        "id": "XeAx9QuuVVE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Answer - # Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Set up the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. Perform GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 7. Predict on the test set and calculate accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 8. Print the results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWawAu6NV6XL",
        "outputId": "1edd3b4d-cc65-4e9c-a7e1-1a4c788d2788"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "We load the Iris dataset and split it into training and test sets.\n",
        "\n",
        "A parameter grid is defined for max_depth and min_samples_split.\n",
        "\n",
        "GridSearchCV tries all combinations using 5-fold cross-validation to find the best parameters.\n",
        "\n",
        "Finally, we evaluate the tuned model on the test set and print the best hyperparameters and accuracy.\n",
        "\n",
        "This approach helps improve model performance while preventing overfitting by finding the most suitable hyperparameters."
      ],
      "metadata": {
        "id": "rURf598CV-uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10** -  Imagine you‚Äôre working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "‚óè Handle the missing values\n",
        "‚óè Encode the categorical features\n",
        "‚óè Train a Decision Tree model\n",
        "‚óè Tune its hyperparameters\n",
        "‚óè Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "**Answer** - 1) Handle missing values ‚Äî step by step\n",
        "\n",
        "Understand the missingness\n",
        "\n",
        "Check patterns: is data MCAR (missing completely at random), MAR (missing at random), or MNAR (not at random)? Use missingness maps and cross-tabulations with other features/target.\n",
        "\n",
        "If missingness correlates with the target or a feature, treat missingness as informative (don‚Äôt blindly drop it).\n",
        "\n",
        "Decide what to drop vs impute\n",
        "\n",
        "Drop columns only if they‚Äôre > say 60‚Äì90% missing and not recoverable or not useful.\n",
        "\n",
        "Drop rows only if very few and no pattern of informativeness.\n",
        "\n",
        "Imputation strategies (choose per column type and context)\n",
        "\n",
        "Numerical: median (robust), mean (if symmetric), or model-based (KNNImputer, IterativeImputer) if relationships exist.\n",
        "\n",
        "Categorical: treat missing as its own category (\"__MISSING__\") or impute with the most frequent value if missingness is non-informative.\n",
        "\n",
        "If missingness is informative: add boolean indicator features (e.g., age_missing = 1) before imputation.\n",
        "\n",
        "Prevent data leakage\n",
        "\n",
        "Fit imputers only on training folds or training set within cross-validation ‚Äî use Pipeline/ColumnTransformer.\n",
        "\n",
        "Advanced: for complex missing patterns use multiple imputation (e.g., IterativeImputer) or domain-specific rules (e.g., lab tests absent because not ordered).\n",
        "\n",
        "2) Encode categorical features ‚Äî step by step\n",
        "\n",
        "Identify variable type: ordinal vs nominal vs high-cardinality.\n",
        "\n",
        "Choose encoding:\n",
        "\n",
        "Nominal with few levels: One-hot encoding (OneHotEncoder(handle_unknown='ignore')) is safe for trees (trees handle sparse, high-dim features okay).\n",
        "\n",
        "Ordinal: OrdinalEncoder with domain order.\n",
        "\n",
        "High-cardinality: frequency encoding, target/mean encoding with out-of-fold (CV) scheme to avoid leakage, or hashing/binary encoding.\n",
        "\n",
        "Avoid leakage: if using target encoding, implement it inside CV or use libraries that support out-of-fold target encoding.\n",
        "\n",
        "Keep missing category if meaningful (see missingness step).\n",
        "\n",
        "Note: Decision trees do not require scaling.\n",
        "\n",
        "3) Train the Decision Tree model ‚Äî step by step\n",
        "\n",
        "Baseline model: build a simple DecisionTreeClassifier(criterion='gini' or 'entropy', random_state=...) to get a baseline.\n",
        "\n",
        "Pipeline: compose preprocessing (imputers + encoders) with the classifier in a single Pipeline so transforms are applied consistently inside CV.\n",
        "\n",
        "Class imbalance: if disease prevalence is low,\n",
        "\n",
        "use class_weight='balanced' or provide sample weights, or\n",
        "\n",
        "try resampling (SMOTE/undersampling) inside a pipeline with careful CV.\n",
        "\n",
        "Feature engineering: create clinically useful features (e.g., BMI from weight/height, lab ratios, time since last visit). Keep clinicians involved.\n",
        "\n",
        "4) Tune hyperparameters ‚Äî step by step\n",
        "\n",
        "Which hyperparameters matter: max_depth, min_samples_split, min_samples_leaf, max_features, max_leaf_nodes, ccp_alpha (cost-complexity pruning), criterion.\n",
        "\n",
        "Search strategy:\n",
        "\n",
        "Start with RandomizedSearchCV to explore wide ranges, then refine with GridSearchCV around good regions.\n",
        "\n",
        "Use StratifiedKFold (maintain target distribution) for CV.\n",
        "\n",
        "Scoring: choose scoring aligned with business goals:\n",
        "\n",
        "For disease detection often prioritize recall (sensitivity) (minimize false negatives) or ROC AUC / PR AUC for imbalanced targets.\n",
        "\n",
        "Use multiple metrics (precision, recall, F1, AUC) for a fuller view.\n",
        "\n",
        "Nested CV: consider nested CV for unbiased generalization estimate if you must pick a model and estimate performance from the same dataset.\n",
        "\n",
        "Calibration: decision trees can produce poorly calibrated probabilities ‚Äî use CalibratedClassifierCV (Platt/isotonic) if probabilities are used for decision thresholds.\n",
        "\n",
        "5) Evaluate performance ‚Äî step by step\n",
        "\n",
        "Train/validation/test split:\n",
        "\n",
        "Reserve a held-out test set (stratified) that is untouched until final evaluation. If data is time-ordered, use time-based split.\n",
        "\n",
        "Primary metrics:\n",
        "\n",
        "Confusion matrix (TP, FP, FN, TN), Precision, Recall (sensitivity), Specificity, F1.\n",
        "\n",
        "ROC AUC and Precision-Recall AUC (PR AUC is often more informative for rare disease).\n",
        "\n",
        "If business has explicit costs, compute expected cost / utility using a cost matrix.\n",
        "\n",
        "Threshold selection:\n",
        "\n",
        "Choose probability threshold based on maximizing business utility (trade-off between recall and precision), not always 0.5.\n",
        "\n",
        "Explainability & trust:\n",
        "\n",
        "Use tree visualization (plot_tree) and feature importances.\n",
        "\n",
        "Use SHAP or LIME for local explanations and to generate rules clinicians can interpret.\n",
        "\n",
        "Robustness & fairness:\n",
        "\n",
        "Test model across subgroups (age, gender, ethnicity) for disparate performance.\n",
        "\n",
        "External validation:\n",
        "\n",
        "Validate on data from a different hospital/time period if available.\n",
        "\n",
        "Monitoring after deployment:\n",
        "\n",
        "Monitor drift (feature distribution, model performance), label distribution, and collect feedback for retraining.\n",
        "\n",
        "6) Deployment, governance, and safety notes\n",
        "\n",
        "Ensure compliance with health data regulations (HIPAA/GDPR), logging and encryption.\n",
        "\n",
        "Involve clinicians to review model rules and flagged high-risk cases.\n",
        "\n",
        "Maintain a feedback loop: capture outcomes to continuously retrain.\n",
        "\n",
        "Build human-in-the-loop workflow for critical decisions ‚Äî model flags cases, clinicians verify.\n",
        "\n",
        "7) Business value (why this model matters)\n",
        "\n",
        "Early detection / triage: identify patients likely to have disease so they can receive timely diagnostic testing or treatment.\n",
        "\n",
        "Resource optimization: prioritize limited diagnostic resources (imaging, specialist referral) to highest-risk patients.\n",
        "\n",
        "Cost savings: reduce expensive late-stage treatments through earlier intervention.\n",
        "\n",
        "Improved outcomes: quicker intervention can improve morbidity/mortality metrics.\n",
        "\n",
        "Operational: automate routine screening, reduce clinician workload for low-risk patients.\n",
        "\n",
        "Explainability: decision trees yield human-readable decision rules that help clinician acceptance and auditability."
      ],
      "metadata": {
        "id": "9akSWYVyWENj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfbioBjwVOv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}